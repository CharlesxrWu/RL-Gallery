# RL Framework
* [Spinning Up, OpenAI](https://spinningup.openai.com/)
* [Rllab, Berkeley RL Lab](https://github.com/rll/rllab)
* [Rllib, UC Berkeleyâ€™s RISE Lab, Ray Project](https://github.com/ray-project/ray/tree/master/python/ray/rllib/)
* [Rlkit, Vitchyr Pong's Implementations](https://github.com/vitchyr/rlkit)
* [Baselines, OpenAI](https://github.com/openai/baselines)
* [Gym, OpenAI](https://gym.openai.com/docs/)
* [Universe, OpenAI](https://blog.openai.com/universe/)
* [Roboschool, OpenAI](https://github.com/openai/roboschool)
* [Reaver, StarCraft II](https://github.com/inoryy/reaver-pysc2)
* [TensorForce](https://github.com/reinforceio/tensorforce)
* [Keras RL](https://github.com/keras-rl/keras-rl)
* [Dopamine, Google](https://github.com/google/dopamine)
* [Coach, Intel](https://github.com/NervanaSystems/coach)
* [PARL, Baidu](https://github.com/PaddlePaddle/PARL)

# RL Tutorials & Blogs & Talks
* [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/bookdraft2017nov5.pdf)
* [CS 294-112, UC Berkeley](http://rail.eecs.berkeley.edu/deeprlcourse/)
* [David Silver's courses](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html)
* [MIT 6.S094: Deep Learning for Self-Driving Cars 2018 Lecture 3 Notes: Deep Reinforcement Learning](https://hackernoon.com/mit-6-s094-deep-learning-for-self-driving-cars-2018-lecture-3-notes-deep-reinforcement-learning-fe9a8592e14a)
* [UCL Advanced Deep Learning & Reinforcement Learning](https://www.youtube.com/playlist?list=PLqYmG7hTraZDNJre23vqCGIVpfZ_K2RZs)
* [An Introduction to Deep Reinforcement Learning](https://arxiv.org/pdf/1811.12560v2.pdf)
* [Morvan Reinforcement Learning](https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/)
* [PyTorch tutorials of RL](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)
* [Arthur Juliani's blog](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0)
* [Thomas Simonini's DRL course](https://www.simoninithomas.com/)
* [Lilian Weng's blog](https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html)
* [Andrej Karpathy's blog](http://karpathy.github.io/2016/05/31/rl/)
* [Hung-yi Lee's DRL lectures (Chinese version)](http://t.cn/RBtg1O2)
* [Spinning Up in Deep RL Workshop, OpenAI](https://www.youtube.com/watch?v=fdY7dt3ijgY)
* [Reproducible, Reusable, and Robust RL, Joelle Pineau](https://www.youtube.com/watch?v=Kee4ch3miVA)

# RL Implementation
* [RL-Adventure, Dulat Yerzat's implementation](https://github.com/higgsfield/RL-Adventure)
* [RL-Adventure2, Dulat Yerzat's implementation](https://github.com/higgsfield/RL-Adventure-2)
* [John Schulman's RL repository](https://github.com/joschu/modular_rl)
* [TianhongDai's implementations](https://github.com/TianhongDai/reinforcement-learning-algorithms)
* [TD3 implementation](https://github.com/sfujim/TD3)
* [SAC implementation](https://github.com/haarnoja/sac)
* [Denny Britz's implementations](https://github.com/dennybritz/reinforcement-learning)
* [steveKapturowski's implementations](https://github.com/steveKapturowski/tensorflow-rl)
* [ShangtongZhang's implementations](https://github.com/ShangtongZhang/DeepRL)
* [DeepPILCO, zuoxingdong's implementation](https://github.com/zuoxingdong/DeepPILCO)
* [Morvan Zhou's implementations](https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow)
* [Sam Greydanus's A3C implementation](https://github.com/greydanus/baby-a3c)
* [Chanwoong joo's implementations](https://github.com/jcwleo/mario_rl)

# RL Papers
* [Model-based-papers](https://github.com/danfeiX/model-based-papers)
* [DRL papers 2015-2016](https://github.com/junhyukoh/deep-reinforcement-learning-papers)
* [Spinning up, OpenAI](https://spinningup.openai.com/en/latest/spinningup/keypapers.html)

# RL Applications
* [Play Atari games, DeepMind](https://arxiv.org/pdf/1312.5602v1.pdf)
* [AlphaGo & AlphaGoZero, playing Go, DeepMind](https://deepmind.com/research/alphago/)
* [AlphaStar, playing StarCraft2, DeepMind](https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/)
* [Autonomous helicopter flight, Stanford University](http://heli.stanford.edu/)
* [Skill learning in 3D simulator, Xue Bin Peng](https://arxiv.org/pdf/1804.02717.pdf)
* [Agile locomotion for quadruped robots, Jie Tan](https://arxiv.org/pdf/1804.10332.pdf)
* [ANYmal robot skill learning, Synced](https://syncedreview.com/2019/01/23/you-cant-keep-an-rl-powered-anymal-down/)
* [Modular legged robot skill learning, Disney Research](https://www.disneyresearch.com/publication/automated-deep-reinforcement-learning-environment-for-hardware-of-a-modular-legged-robot/)
* [RL in business (Chinese version), Alibaba](http://techforum-img.cn-hangzhou.oss-pub.aliyun-inc.com/1517812754285/reinforcement_learning.pdf)
* [LOXM, executing trades, J.P.Morgan](https://www.jpmorgan.com/global/LOXM)
* [ELF, a platform for game research, Facebook](https://github.com/pytorch/ELF)

# RL Technique

| Technique                 | Benefit                                   | Mentioned Key Algorithm                                      |
| ------------------------- | ----------------------------------------- | ------------------------------------------------------------ |
| Target network            | Stabilize the training process            | [DQN, 2015](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)  |
| Memory buffer             | Breaking data relevance                   | [DQN, 2015](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)  |
| KL-constrained update     | Optimize update step size                 | [TRPO, 2015](https://arxiv.org/pdf/1502.05477.pdf)           |
| Advantage function        | Stabilize learning                        | [A3C, 2015](https://arxiv.org/pdf/1602.01783.pdf)            |
| Importance sampling       | Data efficient                            | [PER,2016](https://arxiv.org/pdf/1511.05952.pdf)             |
| Entropy-regularized       | Better exploration                        | [Soft Q-Learning, 2018](https://arxiv.org/pdf/1704.06440.pdf) |
| Boltzmann policy          | Richer mathematical meaning               | [Soft Q-Learning, 2018](https://arxiv.org/pdf/1704.06440.pdf) |
| Target policy smoothing   | Avert Q-function incorrect sharp peak     | [TD3, 2018](https://arxiv.org/pdf/1802.09477.pdf)            |
| Clipped double-Q learning | Fend off overestimation in the Q-function | [TD3, 2018](https://arxiv.org/pdf/1802.09477.pdf)            |
| Reparameterize the policy | Lower variance estimate                   | [SAC, 2018](https://arxiv.org/pdf/1801.01290.pdf)            |

**PS: "Mentioned Key Algorithm" may not be the first algorithm that uses this technique, but makes a detailed explanation**